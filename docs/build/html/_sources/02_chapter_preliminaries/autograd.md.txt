# Autograd

* 设置某个Tensor需要梯度

  `x.requires_grad_(True)`

* 梯度清0

  `x.grad.zero_()`

* 反向传播

  `y.sum().backward()`

> 大多数，对loss函数的常数进行反向传播；
>
> 当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。



## 分离计算

有时，我们希望[**将某些计算移动到记录的计算图之外**]。

例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。

想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，我们希望将`y`视为一个常数，

并且只考虑到`x`在`y`被计算后发挥的作用。

在这里，我们可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，

但丢弃计算图中如何计算`y`的任何信息。

换句话说，梯度不会向后流经`u`到`x`。

因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，

而不是`z=x*x*x`关于`x`的偏导数。

* detach

  当前计算阶段不考虑梯度

  `y.detach()`

```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x
z.sum().backward()
x.grad == u
```



u是y的一个拷贝，u不计算梯度，但x的梯度依然保留在y里面。
